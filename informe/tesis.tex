\documentclass{llncs}
\bibliographystyle{splncs}
\renewcommand{\contentsname}{índice general}
\renewcommand\refname{Bibliografía}
\renewcommand\abstractname{Resumen}
\usepackage{listings}
\usepackage{indentfirst, enumitem,amsmath}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[usenames, dvipsnames]{color}
\usepackage{comment}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\setcounter{secnumdepth}{3}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\makeatletter
\newenvironment{breakablealgorithm}
{% \begin{breakablealgorithm}
	\begin{center}
		\refstepcounter{algorithm}% New algorithm
		\hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
		\renewcommand{\caption}[2][\relax]{% Make a new \caption
			{\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
			\ifx\relax##1\relax % #1 is \relax
			\addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
			\else % #1 is not \relax
			\addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
			\fi
			\kern2pt\hrule\kern2pt
		}
	}{% \end{breakablealgorithm}
		\kern2pt\hrule\relax% \@fs@post for \@fs@ruled
	\end{center}
}
\makeatother


% Titulo
\title{Selección de Componentes Discretos para un Filtro Activo Mediante
  \textit{Constraint Programming}} 
\author{Leandro Demarco Vedelago}
\institute{
            \email{leandrodemarco@gmail.com}\\
            Universidad Nacional de Córdoba, Fa.M.A.F
          }
\setcounter{tocdepth}{3}
\begin{document}
{\def\addcontentsline#1#2#3{}\maketitle
  \noindent
  \makebox[\linewidth]{\small 25 de Marzo de 2017}}
  
\begin{abstract}
  Esto es el resumen 
\end{abstract}
% Fin titulo

%Tabla de contenidos
\tableofcontents
\newpage
%Fin tabla de contenidos
  \section{\textbf{Motivación}}
    \label{sec:motivacion}
    El dise\~no electrónico actual incluye a los filtros activos en muchas aplicaciones,
    tales como acondicionamiento y manipulación de se\~nales en frecuencias de audio e
    intermedias (IF) así como tareas de procesamiento digital de se\~nales. En
    contraposición a los filtros digitales, los activos pueden obtener buena performance
    con demandas de potencia significativamente menores.
    
    Las alternativas de implementación de filtros activos presentan muchas opciones.
    Entre éstas, las implementaciones RC (resistencia/capacitor), construidas a partir de
    amplificadores operacionales, resistencias y capacitores son una de las más utilizadas
    por los ingenieros.\cite{corr}
    
    Un aspecto que cobra relevante importancia es la selección de los componentes discretos
    ya que el cumplimiento de las especificaciones depende en gran medida de ellos. Con el
    objetivo de realizar un diseño confiable los valores de los componentes pasivos se
    seleccionan de entre las series industriales E12, E24, E48, E96 o E192. Cada una de estas
    series limita el valor que puede tomar cada componente.
    
    A fin de obtener un diseño que satisfaga las especificaciones,  una posibilidad consiste 
    en enumerar todas las posibles combinaciones de valores para las resistencias y 
    capacitores que conforman el filtro y encontrar aquella que mejor las cumple. Sin embargo,
    dada la amplitud del rango de valores que cada componente puede adoptar el tamaño del
    espacio de soluciones suele ser inmanejable para adoptar este enfoque, con
    lo cual es necesario utilizar métodos o heurísticas de búsqueda que reduzcan el tiempo
    necesario.
    
    En este trabajo en la sección \ref{sec:fundelect} se define el filtro analizado
    junto a los fundamentos electrónicos que definen la bondad del mismo. Luego, en la
    sección \ref{sec:fundprog} se introduce la noción de \textit{Constraint Programming}
    y cómo se define en términos de este paradigma el problema en cuestión. En la sección
    \ref{sec:exhaustSearch} se presenta una serie de observaciones y manipulaciones matemáticas del problema que permiten definir un algoritmo exhaustivo que se ejecuta
    en tiempos razonables. Finalmente, en la sección \ref{sec:acor} se presenta la metaheurística ACO
    -del inglés \textit{Ant Colony Optimization}- aplicada a este problema junto a una variante 
    del mismo llamada ACO-$\mathbb{R} $
    
    
  \section{\textbf{Fundamentos electrónicos}}
    \label{sec:fundelect}
    Se tomará como caso de estudio el mismo filtro que se utiliza en \cite{lov:rom:per}. Dicho filtro
    es de tipo IGMFB (Infinite-Gain Multiple Feedback) pasabajo de segundo orden. Son filtros
    bicuadráticos que emplean múltiples lazos de realimentación y un amplificador operacional.
    Una descripción más detallada de este tipo de filtros puede encontrarse en \cite{dim} y \cite{rau:swa}. En la figura \ref{fig:filter} se muestra el esquemático del filtro a analizar.
   
   \begin{figure}
   	\includegraphics[width=\linewidth]{filter.png}
   	\caption{Filtro IGMFB pasabajo de segundo orden.}
   	\label{fig:filter}
   \end{figure}

	Las especificaciones del filtro están dadas por la ganancia en la banda de paso ($G$), la
	frecuencia de polo ($\omega_p$) y el factor de calidad ($Q_p$). En función de estos valores
	puede definirse la función de transferencia del filtro.
 	\begin{equation}
		F(s) = \frac{G\omega_p^2}{s^2+(\frac{\omega_p}{Q_p})s+\omega_p^2}
		\label{funcTransfer}
	\end{equation}
	
	Para los filtros IGMFB los valores de $G$, $\omega_p$ y $Q_p$ pueden calcularse a partir
	de los valores de las resistencias y capacitores, de acuerdo a las siguientes fórmulas
	
	\begin{eqnarray}
		G &=& R_2/R_1 \label{g}\\
		\omega_p &=& 1/\sqrt{R_2 R_3 C_4 C_5} \label{omega}\\
		Q^{-1}_p &=& \sqrt{C_5/C_4}  \left(\sqrt{R_2 R_3}/R_1 + \sqrt{R_3/R_2} +
		\sqrt{R_2/R_3}\right) \label{q}
	\end{eqnarray}
	
	\subsection{Sensibilidad de un filtro IGMFB}
	El término sensibilidad es utilizado para expresar una medida de
	la variación de la performance como resultado de cambios en los valores de los
	componentes. Dichas variaciones pueden ocurrir debido al envejecimiento de los
	mismos, tolerancias de fabricación, condiciones ambientales (temperatura), entre
	otros factores \cite{dim,rau:swa}. Mientras menos sensible es un filtro a los cambios en sus
	componentes, más estables permanecen sus características y, por lo tanto, existen más
	probabilidades de que pueda permanecer dentro de sus especificaciones,
	independientemente de la presencia de dichos cambios. Una ventaja de los filtros IGMFB es
	que presentan sensibilidades más bajas respecto a otros filtros bicuadráticos.
	
	De manera general, si $F$ es una función de varias variables, $F(x_1, x_2,...,x_n)$, entonces
	la sensibilidad de F con respecto a $x_i$ está definida por:
	\begin{equation}
		S_{x_i}^F = \frac{\%\ cambio\ en\ F}{\%\ cambio\ en\ x_i} = \frac{\partial F/F}{\partial x_i/x_i}
		\label{sensibilidad}
	\end{equation}
	
	Se considera que un filtro tiene baja sensibilidad cuando la sensibilidad respecto a cada uno
	de sus componentes es inferior a la unidad \cite{rau:swa}.
	
	Para los filtros pasabajo IGMFB las sensibilidades de $Q_p$ y $\omega_p$ con respecto a cada uno de los componentes pasivos vienen dadas por las siguientes ecuaciones:
	
	\begin{eqnarray}
		S_{R_1}^{\omega_p} &=& 0 \label{sensR1Omega}\\
		S_{R_2}^{\omega_p} = S_{R_3}^{\omega_p} &=& S_{C_4}^{\omega_p} = S_{C_5}^{\omega_p} = -\left(\frac{1}{2}\right) \label{sensOtrosOmega}\\
		S_{C_4}^{Q_p} &=& - S_{C_5}^{Q_p} = \left(\frac{1}{2}\right) \label{sensCapQ}\\
		S_{R_1}^{Q_p} &=& Q_p \left(\frac{1}{R_1} \sqrt{\frac{R_2 R_3 C_5}{C4}}\right) \label{sensR1Q}\\
		S_{R_2}^{Q_p} &=& - \frac{Q_p}{2} \left(\frac{1}{R1} \sqrt{\frac{R_2 R_3 C_5}{C_4}} - \sqrt{\frac{R_3 C_5}{R_2 C_4}} + \sqrt{\frac{R_2 C_5}{R_3 C_4}}\right) \label{sensR2Q}\\
		S_{R_3}^{Q_p} &=& - \frac{Q_p}{2} \left(\frac{1}{R1} \sqrt{\frac{R_2 R_3 C_5}{C_4}} + \sqrt{\frac{R_3 C_5}{R_2 C_4}} - \sqrt{\frac{R_2 C_5}{R_3 C_4}}\right) \label{sensR3Q}
	\end{eqnarray}
	
	Como se puede ver, en (\ref{sensR1Omega}), (\ref{sensOtrosOmega}) y (\ref{sensCapQ}), las
	sensibilidades adoptan valores fijos, mientras que las otras dependen de los valores que adopten los componentes. En consecuencia, debemos considerar las sensibilidades (\ref{sensR1Q}), (\ref{sensR2Q}) y (\ref{sensR3Q}) a la hora de seleccionar los valores para cada componente.
	En el caso de estudio seleccionado, la especificación elegida para el filtro es $G_F = 3$,
	$\omega_{p_F}=1000*2*\pi = 6283,9478\ rad/s$ y $Q_{p_F} = 0,707$. 
	
	Consideraremos así mismo un escenario donde las resistencias y capacitores pueden tomar valores de la serie E24 y E12, con rangos en $10^3$-$10^6$$\Omega$ y $10^{-9}$-$10^{-6}$F, respectivamente. Se asume que valores fuera de estos rangos conducirían a efectos negativos debido a capacidades parásitas o señales de corriente muy grandes. De esta manera, el espacio de búsqueda total asciende a 4,84E08 alternativas. Se
	define además una tolerancia o error máximo $E_{max} = 2,5\%$ para los valores de $G_F$,
	$\omega_{p_F}$ y $Q_{p_F}$.
	
    
  \section{\textbf{\textit{Constraint Programming}}}
    \label{sec:fundprog}
    Intuitivamente, los problemas de satisfacción de restricciones (\textit{CSP}, por sus siglas en inglés) pueden definirse como \textit{encontrar una solución que satisface determinadas restricciones o pro\-pie\-da\-des}. Muchos problemas de la vida real pueden ser reducidos a 
    este tipo de problemas, por ejemplo: el planeamiento y control del tráfico aéreo en un aeropuerto, el confeccionamiento de una agenda de clases o el diseño de una dieta saludable. 
    Todos estos problemas suelen ser \textit{NP}-Complejos.
    
   Existen dos técnicas para intentar solucionar este tipo de problemas \cite{sol}: una posibilidad es utilizar \textit{métodos exactos}, que exploran el espacio de combinaciones de forma exhaustiva estructurándolo como un árbol de búsqueda. Para acotar el problema de la
    explosión combinatoria, la búsqueda se combina con técnicas de filtrado, para
    \textit{podar} subconjuntos de combinaciones, y con heuristícas de ordenación, para orientar la búsqueda hacia las ramas más promisorias primero.
    
    Cuando las técnicas de filtrado y las heurísticas de ordenamiento no son suficientes para prevenir la explosión combinatoria, se debe dejar de lado la exhaustividad y utilizar \textit{métodos heurísticos} que exploran el espacio de búsqueda de forma incompleta, utilizando (meta)heurísticas para guiar la búsqueda hacia las áreas más promisorias ignorando deliberadamente otras áreas. Dentro de este enfoque hay dos variantes: las \textit{heurísticas perturbadoras} -ej: Algoritmos Genéticos- que van modificando iterativamente combinaciones existentes a fin de crear nuevas combinaciones, y las \textit{heurísticas constructivas} que construyen nuevas combinaciones iterativamente desde cero.
    
    \subsection{Noción de una restricción}
      Una restricción puede pensarse como una relación lógica entre un conjunto de valores, en principio,
      desconocidos, también llamados \textit{variables}. De esta forma, una restricción restringe el conjunto
      de valores que pueden asignarse simultaneamente a sus variables.
      
      Una posibilidad para definirla consiste en enumerar todo el conjunto de tuplas que pertenece a la relación o,
      dualmente, especificar aquellos que \textbf{no} pertenecen a ella.
      
      Existen diferentes tipos de restricciones, de acuerdo a los valores que pueden asignarse a las variables. Así,
      tenemos restricciones numéricas, booleanas, sobre conjuntos, etc. Una restricción numérica puede ser entonces
      una igualdad ($=$) o bien una desigualdad ($\neq, \geq, \leq, <, >$) entre dos expresiones aritméticas.
      
      Similarmente, las restricciones sobre conjuntos expresan relaciones entre variables cuyo rango son conjuntos y pueden
      incluir relaciones de igualdad ($=$), desigualdad ($\neq$), inclusión ($\subset, \subseteq$) o pertenencia ($\in$).
      
      Las restricciones sobre booleanos, pueden expresarse en términos de los operadores
      lógicos como por ejemplo $\wedge$, $\lor$, $\neg$, $\rightarrow$, etc.
      
    \subsection{Definición formal de un CSP}
    \label{subsec:cspformal}
      Se puede dar una definición formal de un CSP como sigue:
      
      Un CSP es una 3-upla (\textit{X}, \textit{D}, \textit{C}) tal que:
      \begin{itemize}
        \item \textit{X} es un conjunto finito de variables que corresponde a las incógnitas del problema.
        \item \textit{D} es una función que asocia un dominio \textit{D}($x_i$) con cada variable $x_i \in \textit{X}$, es
        decir \textit{D}($x_i$) es el conjunto de valores que la variable $x_i$ puede tomar.
        \item \textit{C} es un conjunto finito de restricciones y cada restricción $c_j \in C$ es una relación entre
        algunas variables de \textit{X}; este conjunto se denota como $var(c_j).$
      \end{itemize}
  
  	Resolver un CSP $(X, D, C)$ consiste en asignar valores a todas las variables de forma tal
  	que todas las restricciones sean satisfechas. Más formalmente, se definen:
  	
  	- Una \textit{asignación} es un conjunto de pares (variables, valor) denotado
  	\begin{center}
  		{$A = \{(x_1, v_1), ..., (x_n, v_n)\}$} 
  	\end{center}
  	donde la misma variable no puede asignarse a dos valores distintos, es decir:
  	\begin{center}
  		{$\forall((x_i, v_i), (x_j,v_j)) \in A \times A,\ x_i = x_j \implies v_i = v_j$}
  	\end{center}
  	y el valor asignado a una variable pertenece a su dominio:
  	\begin{center}
  		{$\forall(x_i, v_i) \in A,\ v_i \in D(x_i)$}
  	\end{center}
     Se denota al conjunto de variables que tienen un valor asignado en la asignación $A$ como
     $var(A)$:
     \begin{center}
     	$var(A) = \{x_i \mid (x_i, v_i) \in A\}$
     \end{center}
 
   - Una asignación $A$ se dice \textit{completa} si asigna un valor a todas las variables del
   problema, es decir, si $var(A) = X$; en caso contrario se dice que es \textit{parcial}.
   
   - Una asignación $A$ \textit{satisface} una restricción $c_k$ tal que $var(c_k) \subseteq var(A)$ si la relación definida por $c_k$ es verdadera para los valores de las variables de $c_k$ definidos en $A$. Caso contrario, la asignación \textit{viola} la restricción.
   
   - Una asignación $A$ (completa o parcial) es \textit{consistente} si satisface todas las restricciones y es \textit{inconsistente} si viola al menos una.
   
   - Una \textit{solución} es una asignación completa y consistente.
   
   \subsection{Complejidad de un CSP}
   \label{subsec:cspComplexity}
   	Dado que los dominios pueden ser intervalos numéricos continuos, no todos los CSP son
   	problemas combinatorios. La complejidad teórica de un CSP depende del dominio de las variables y del tipo de restricciones utilizadas.
   	
   	En algunos casos, el problema puede ser polinomial. Este es caso, por ejemplo, cuando todas
   	las restricciones son (in)ecuaciones lineales y todos los dominios son intervalos numéricos
   	continuos.
   	
   	En otros casos, el problema puede ser indecidible. Por ejemplo, cuando las restricciones puede ser cualquier fórmula matemática arbitraria y los dominos de las variables son intervalos numéricos continuos.
   	
   	Sin embargo, en muchos casos, el problema es $\mathcal{NP}$-completo. En particular, los CSP con dominios finitos suelen pertenecer a este grupo en general.
   	
   	\subsection{Problemas de optimización relacionados a CSP}
   	\label{subsec:cspOptimization}
   	Los problemas de satisfacción de restricciones implican encontrar una solución que las
   	satisfaga a todas o bien probar la inconsistencia si no existe ninguna solución. Sin embargo,
   	en muchos casos también suele haber involucrados problemas de optimización. En particular,
   	el resolver problemas \textit{excesivamente restringidos} suele convertirse en maximizar la
   	satisfacción de restricciones. Otras veces el resolver un CSP tambien involucra optimizar
   	alguna función objetivo al tiempo que se satisfacen todas las restricciones.
   	
   	\subsubsection{Maximización de satisfacción de restricciones}
   	Cuando las restricciones de un CSP son tales que no pueden ser todas satisfechas simultáneamente, se dice que el CSP está \text{excesivamente restringido}. En este caso,
   	usualmente se intenta hallar una asignación completa que satisfaga tantas restricciones como sea posible o, a la inversa, viole la menor cantidad posible. Este problema se llama \textit{CSP parcial} o \textit{MaxCSP} \cite{fre:wal}.

	También suele ocurrir en muchos problemas que no todas las restricciones sean igualmente
	importantes. Algunas pueden ser \textit{duras}, con lo cual no deben ser violadas, mientras
	que otras pueden ser \textit{blandas}, permitiendo que se las viole a algún costo dado. En este caso se le asocia un peso a cada restricción blanda que define el costo de violarla, y el objetivo es encontrar la asignación completa que satisface todas las restricciones duras y minimiza la suma ponderada de las restricciones blandas violadas. Este problema se llama CSP ponderado, (WCSP, por sus siglas en inglés) \cite{shi:far:ver}.
	
	Para estos CSP excesivamente restringidos, el espacio de búsqueda está definido por el
	conjunto de todas las asignaciones completas posibles y el objetivo es encontrar aquella que maximiza el nivel de satisfacción: el número de restricciones satisfechas en el caso de MaxCSP y
	la suma ponderada de las restricciones satisfechas en el caso los WCSP. En la mayoría de los
	casos, estos problemas son $\mathcal{NP}$-complejos ya que los problemas de decisión asociados son $\mathcal{NP}$-completos. Debe notarse además que son generalizaciones de
	CSP, con lo cual un algoritmo diseñado para resolver cualquiera de estos problemas puede
	utilizarse para resolver un CSP.
	
	\subsubsection{Optimización restringida}
	Cuando las restricciones de un CSP son tales que existen múltiples soluciones que las satisfacen a todas, el CSP se dice estar \textit{sub-restringido}. En este caso, algunas soluciones
	pueden ser preferibles a otras. Estas preferencias pueden ser expresadas añadiendo una función
	objetivo a ser maximizada (o minimizada), definiendo así un un problema de optimización
	restringida. Formalmente, un problema de optimización restringida está definido por un CSP
	$(X, D, C)$ y una función objetivo $f: X \rightarrow \mathbb{R}$. El objetivo es hallar una solución del CSP que maximiza (o minimiza) $f$. En estos casos la dificultad no radica en hallar
	una solución, si no en hallar una óptima con respecto a $f$.
	
	\subsection{Definición del problema como CSP}
		\label{subsec:problemDefinition}
		Como se estableció en la sección \ref{sec:fundelect}, el problema consiste en seleccionar
		los valores para las resistencias $R_1$, $R_2$ y $R_3$ y los capacitores $C_4$ y $C_5$ de
		una lista de posibles valores (las series industriales E). Si defino
		
		\begin{align*}
		G^{max} = G_F * (1 + E_{max})\\
		G^{min} = G_F * (1 - E_{max})\\
		\omega_p^{max} = \omega_{p_F} * (1 + E_{max})\\
		\omega_p^{min} = \omega_{p_F} * (1 - E_{max})\\
		Q_p^{max} = Q_{p_F} * (1 + E_{max})\\
		Q_p^{min} = Q_{p_F} * (1 - E_{max})
		\end{align*}
		
		Entonces el problema puede modelarse como un CSP $(X, D, C)$ donde:
		
		\begin{itemize}
			\item $X = \{R_1, R_2, R_3, C_4, C_5\}$
			\item $D(R_i) = E24, i \in \{1,2,3\}$ y $D(C_i) = E12, i \in \{4,5\}$
			\item $C = \{G^{max} > G_F > G^{min}, \omega_p^{max} > \omega_{p_F} > \omega_p^{min},
			Q^{max} > Q_F > Q^{min}\}$
		\end{itemize}
    

  \section{\textbf{Algoritmo exhaustivo}}
  	\label{sec:exhaustSearch}
    Realizando algunas observaciones y transformaciones matemáticas, es posible obtener un algoritmo exhaustivo que descubre
    todas las soluciones posibles para el problema. Recordemos las definiciones
    de \textit{ganancia} ($G$), \textit{frecuencia de polo} ($\omega_p$) y
    \textit{factor de calidad} ($Q_p$)


    Es claro que a partir de (\ref{g}) se tiene que para cada valor de $R_1$ hay
    un conjunto de valores de $R_2$ que denotaremos 
    $\{R_2\}_{R_1}$ tales que $G^{max} > G > G^{min}$

    Definiendo las siguientes cantidades,
    \begin{eqnarray}
      \omega_1 &=& 1/(R_2 C_4) \nonumber \\
      \omega_2 &=& 1/(R_3 C_5)
      \label{omegas}
    \end{eqnarray}
    podemos reescribir (\ref{omega}) de la siguiente forma: $\omega_p = \sqrt{\omega_1  \omega_2}$

    
    A partir de los valores que pueden tomar las resistencias y capacitores, es
    posible calcular la lista de valores que pueden tomar las frecuencias
    definidas en (\ref{omegas}). Para cada uno de los valores posibles para
    $\omega_1$, habrá un conjunto de pares de valores ($R_2$, $C_4$) tales que
    $\omega_1 = 1/(R_2 C_4)$. Denotamos a tal conjunto como $\{(R_2,
      C_4)\}_{\omega_1}$. De manera análoga se puede proceder con $\omega_2$,
      $R_3$ y $C_5$ (obsérvese el conjunto de valores posibles para $\omega_2$
      es el mismo que para $\omega_1$ y que $\{(R_2,C_4)\}_{\omega_1}=\{(R_3,
      C_5)\}_{\omega_2}$ cuando $\omega_1=\omega_2$). Luego, dada la restricción $\omega_p \in 
    [\omega_p^{min},\omega_p^{max}]$ y elegido un valor de  $\omega_1$, los
    valores admisibles de $\omega_2$ son aquellos para los 
    que $\omega_p= \sqrt{\omega_1 \omega_2}$ satisface dicha restricción, y
    por lo tanto cumplen las siguiente relación:
    $(\omega_p^{max})^2 / \omega_1 > \omega_2 > (\omega_p^{min})^2 / \omega_1 $.

    De esta manera, para cada valor realizable de $\omega_1$ existe un conjunto
    finito $\{{\omega_2}\}_{\omega_1}$ de valores de $\omega_2$ 
    tales que $ \omega_p^{max} > \omega_p > \omega_p^{min} $

    Finalmente, a partir de (\ref{q}) y (\ref{omegas}) podemos reescribir
    $Q_p^{-1}$ de la siguiente forma,
    \begin{equation}
    Q_p^{-1} =
    \sqrt{\frac{\omega_1}{\omega_2}}\left(1+\frac{R_2}{R_1}+\frac{R_2}{R_3}\right)
    \label{otraq}
    \end{equation}
    
    Finalmente, para cada par ($R_1$, $R_2$), ($\omega_1, \omega_2$) que satisface
    las restricciones sobre $G$ y $\omega_p$, utilizando la expresión
    (\ref{otraq}) buscamos
    los valores $R_3$ admisibles que satisfagan la 
    condición
    $$
    (Q_p^{-1})^{max} > Q_p^{-1} > (Q_p^{-1})^{min}
    $$
    y de esta manera obtenemos todas las soluciones de la forma ($R_1$, $R_2$,
    $\omega_1$, $\omega_2$, $R_3$)  
    que luego podemos \textit{mapear} a las soluciones de la forma ($R_1$,
    $R_2$, $R_3$, $C_4$, $C_5$). 

	\begin{breakablealgorithm}
		\caption{Búsqueda exhaustiva}
		\label{alg:exhaustSearch}
		\begin{algorithmic}[1]
			\State $R_{vals}$: conjunto de valores que pueden tomar las resistencias
			\State $C_{vals}$: conjunto de valores que pueden tomar los capacitores
			\State $G^{min}$: valor mínimo aceptable para la ganancia
			\State $G^{max}$: valor máximo aceptable para la ganancia
			\State $\omega_p^{min}$: valor mínimo aceptable para la frecuencia de polo
			\State $\omega_p^{max}$: valor máximo aceptable para la frecuencia de polo
			\State $Q^{min}$: valor mínimo aceptable para el factor de calidad
			\State $Q^{max}$: valor máximo aceptable para el factor de calidad
			\item[]
			\Procedure{ExhaustiveSearch}{$R_{vals}$, $C_{vals}$}
			\State $soluciones \gets []$
			\State $G_{constraint} \gets  []$ \Comment{Obtener pares $(r1,r2)$ que satisfagan la restricción sobre G}
			\For{cada $r1$ en $R_{vals}$, cada $r2$ en $C_{vals}$}
			\State $G \gets r2/r1$
			\If{$G^{max} > G > G^{min}$}
			\State Agregar el par $(r1, r2)$ a $G_{constraint}$
			\EndIf 
			\EndFor
			\item[]
			\State $\omega_{constraint} \gets \{\}$ \Comment{Diccionario que para cada $\omega_1$ contiene el conjunto de los $\omega_2$ posibles para no violar la
			restricción}
			\State $wToRCMap \gets \{\}$  \Comment{Diccionario que para cada valor de $w$ contiene una lista con los pares $(r,c)$ que lo generan}
			\item[]
			\State $w_{vals} = [ ]$
			\For{cada $r$ en $R_{vals}$, cada $c$ en $C_{vals}$}
			\State $w \gets 1/(r*c)$
			\State Agregar el par $(r,c)$ a $wToRCMap[w]$
			\State Agregar $w$ a la lista $w_{vals}$
			\EndFor
			\For{cada $w_1$ en $w_{vals}$}
			\State $Posibles\_\omega_2 \gets [ ]$
			\For{cada $w_2$ en $w_{vals}$}
			\State $minVal \gets {(\omega_p^{min})}^{2}$
			\State $maxVal \gets {(\omega_p^{max})}^{2}$
			\If{$maxVal > w2 > minVal$}
			\State Agregar $w2$ a la lista $Posibles\_\omega_2$
			\EndIf
			\EndFor
			\If{$Posibles\_\omega_2$ no es vacía}
			\State Agregar $Posibles\_\omega_2$ a $\omega_{constraint}[\omega_1]$
			\EndIf
			\EndFor
			\item[]
			\For{cada par $(r1,r2)$ en $G_{constraint}$}
			\For{cada $\omega_1$ en las keys de $\omega_{constraint}$}
			\State $Posibles\_\omega_2 \gets \omega_{constraint}[\omega_1]$
			\State $genera \gets \exists (r,c) \in wToRCMap[\omega_1] \mid r = r_2$ \Comment{Chequear si $r_2$ \textit{genera} $\omega_1$}
			\If{$genera$}
			\For{cada $\omega_2$ en $Posibles\_\omega_2$, cada $r_3$ en $R_{vals}$}
			\State $genera \gets \exists (r,c) \in wToRCMap[\omega_2] \mid r = r_3$ \Comment{Chequear si $r_3$ \textit{genera} $\omega_2$}
			\If{$genera$}
			\State $(Q^{-1})_{r_3} \gets \sqrt{\omega_1 / \omega_2} * (1  + r_2/r_1 + r_2/r_3)$
			\If{$(Q^{min})^{-1} > (Q^{-1})_{r_3} > (Q^{max})^{-1}$}
			\State $c_4 \gets 1/(r_2 * \omega_1)$
			\State $c_5 \gets 1/(r_3 * \omega_2)$ 
			\State Agregar la tupla $(r_1, r_2, r3, c4, c5)$ a la lista $soluciones$
			\EndIf
			\EndIf
			\EndFor
			\EndIf
			\EndFor
			\EndFor
			\State \textbf{return} $soluciones$
			\EndProcedure
		\end{algorithmic}
	\end{breakablealgorithm}

  \section{\textbf{La Metaheurística ACO-$\mathbb{R} $}}
  \label{sec:acor}
  La Optimización por Colonia de Hormigas -en inglés \textit{Ant Colony Optimization}- es una metaheurística introducida en los años '90 inspirada en el comportamiento de las hormigas
  para solucionar problemas de optimización combinatoria \cite{dor92,dor:man:col,dor:schu}.
  
  Al buscar comida, las hormigas inicialmente exploran el área circundante a su nido de manera aleatoria. En cuanto una hormiga encuentra una fuente de comida, la evalúa y lleva un poco hacia el nido. Durante el viaje de vuelta, la hormiga va depositando feromona en el suelo. La cantidad de feromona depositada depende de la cantidad y calidad de comida sirve para guiar otras hormigas hacia la fuente de comida. Esta forma de comunicación indirecta a través de la feromona  les permite hallar los caminos más cortos entre el nido y la fuente de comida \cite{gos:aro:den:pas}. Este comportamiento ha inspirado la definición de colonias artificiales de hormigas que pueden encontrar soluciones aproximadas -en el sentido que son buenas soluciones pero no necesariamente óptimas- a problemas de optimización combinatorial complejos.
  
  \subsection{La metaheurística ACO}
  En el algoritmo \ref{alg:acoMetaheuristic} se presenta la metaheurística ACO, que consiste de tres
  actividades. A continuación se detallan cada una de estas actividades.
  
  	\begin{breakablealgorithm}
  	\caption{Metaheurística ACO}
  	\label{alg:acoMetaheuristic}
  	\begin{algorithmic}[2]
  		\While{condiciones de terminación no alcanzadas}
  		\State ConstruirSolución()
  		\State ActualizarFeromonas()
  		\State AccionesDaemon() \Comment{Opcional}
  		\EndWhile
  	\end{algorithmic}
	\end{breakablealgorithm}

	Recordemos de la sección \ref{subsec:cspformal} que en un CSP $(X, D, C)$ cada variable
	$x_i$ del problema en $X$, tiene un dominio $D(x_i)$ indicando los valores que puede tomar
	dicha variable. Definimos una \textit{componente de solución} $c_{ij}$ a la asignación $x_i = v_j$ donde $v_j \in D(x_i)$. Y definimos también $C_s$ como el conjunto de todas las componentes de solución posibles.
	
	\bigbreak
	\textit{ConstruirSolución()}: Las hormigas artificiales construyen soluciones creando una secuencia de componentes elegidas de un conjunto finito de valores posibles para cada componente. 
	
	La construcción de esta secuencia comienza con una solución parcial vacía 
	$s^p = \emptyset$. Luego, en cada paso de la construcción, $s^p$ se extiende agregando una
	componente perteneciente al conjunto $N(s^p) \in C_s \setminus s^p$, que está definido por por el mecanismo de construcción de la solución.
	
	El proceso de construir una solución puede ser considerado como un camino en un grafo
	de construcción $G_C = (V, E)$ donde el conjunto de componentes de solución $C_s$ está
	asociado o bien con el conjunto de vértices $V$ del grafo, o bien con el de sus aristas $E$. Los caminos permitidos en $G_C$ están definidos implícitamente por el mecanismo de construcción de la solución que define el conjunto $N(s^p)$ respecto a la solución parcial $s^p$.
	
	La elección de una componente de solución del conjunto $N(s^p)$ se realiza de forma probabilista en cada paso de la construcción. Las reglas exactas para esta elección varían entre
	cada una de las diferentes variantes de ACO. Una de las más conocidas es la que utiliza \textit{Ant System} \cite{dor:man:col}:
	
	\begin{equation}
	\label{eq:chooseComponent}
	p(c_{ij} \mid s^p) = \frac{\tau_{ij}^\alpha \cdot \eta(c_{ij})^\beta}{\sum_{c_{il} \in N(s^p)} \tau_{il}^\alpha \cdot \eta(c_{il})^\beta}, \forall c_{ij} \in N(s^p)
	\end{equation}

	donde $\tau_{ij}$ es la cantidad de feromona asociada a la componente $c_{ij}$ y $\eta(\cdot)$
	es una función de ponderación que en cada paso de la construcción le asigna un valor heurístico a cada componente $c_{ij} \in N(s^p)$. Los valores que devuelve la función de ponderación suelen llamarse \textit{información heurística}. $\alpha$ y $\beta$ son parámetros con valores positivos que determinan la relación entre la información obtenida de las feromonas y la heurística.
	\bigbreak
	
	\textit{ActualizarFeromonas()}: El objetivo de la actualización de feromonas es incrementar el valor asociado con soluciones buenas o promisorias, y decrementar el valor para soluciones malas. Usualmente esto se consigue aumentando los niveles de feromona asociados a una buena solución escogida $s_{ch}$ en un determinado valor $\Delta\tau$ y reduciendo todos los valores de feromona a través del mecanismo de \textit{evaporación de feromonas}:
	\begin{equation*}
		\tau_{ij} \gets
		\begin{cases}
		(1 - \rho) \tau_{ij} + \rho\Delta\tau,& \text{si } \tau_{ij}\in s_{ch}\\
		(1 - \rho) \tau_{ij},              & \text{caso contrario}
		\end{cases}
	\end{equation*} 
	
	donde $\rho \in \left(0,1\right]$ es la \textit{tasa de evaporación}. Este mecanismo de evaporación es necesario para evitar una convergencia demasiado rápida del algoritmo. Implementa una útil manera de \textit{olvidar}, para favorecer la exploración de nuevas áreas del espacio de búsqueda.
	
	En general, las buenas soluciones halladas tempranamente por las hormigas se utilizan para actualizar las feromonas de forma tal que se aumente la probabilidad de búsqueda por hormigas que las siguen en las áreas promisorias del espacio de búsqueda. Cada variante de ACO suele implementar distintas formas de actualización de feromonas. En principio, pueden escoger entre dos estrategias: basándose en la mejor solución encontrada en la última iteración, o bien en la mejor solución encontrada hasta el momento, desde que el algoritmo comenzó a ejecutarse. La primera favorece una mayor exploración del espacio de búsqueda, mientras que la segunda lleva a una convergencia más rápida \cite{stu:dor}.
	\bigbreak
	
	\textit{AccionesDaemon()}: Son acciones que pueden ser usadas para implementar acciones centralizadas que las hormigas no puedan realizar individualmente. Por ejemplo, aplicar búsqueda local a las soluciones encontradas, o recolectar información global que puede ser utilizada para decidir si es útil o no depositar feromona adicional para influir el proceso de búsqueda desde una perspectiva no local.

	\subsection{ACO en dominios continuos: ACO-$\mathbb{R} $}
	\label{subsec:acor}
	Dado un CSP $(X,D,C)$ diremos que el mismo tiene \textit{dominio continuo} si $D_i \subseteq \mathbb{R}, \forall x_i \in X$.
	
	La idea central a la forma en la que ACO trabaja es la construcción incremental de soluciones basadas en la selección probabilista -influenciada por la feromona- de componentes de la solución. Cuando se aplica ACO a problemas de optimización combinatorios, el conjunto de de las componentes de solución está definido por la formulación del problema. En cada paso de la construcción, la hormiga escoge de manera probabilista una componente $c_{ij} \in N(s^p)$ de acuerdo a la ecuación (\ref{eq:chooseComponent}). Las probabilidades asociadas con los elementos del conjunto $N(s^p)$ de componentes disponibles  forman una distribución de probabilidad \textit{discreta} que cada hormiga muestrea para escoger la componente a agregar a la solución parcial $s^p$.
	
	En ACO-$\mathbb{R}$, la idea fundamental es pasar de utilizar esta distribución discreta a utilizar una \textit{continua}, es decir, una función de densidad de probabilidad.
	
	\subsubsection{\textit{La función de densidad de probabilidad}}
	Una \textit{función de densidad de probabilidad} (FDP) puede ser cualquier función $P(x) \geq 0, \forall x$ tal que
	
	\begin{equation*}
	\int_{-\infty}^{\infty}P(x)dx = 1
	\end{equation*}
	
	Para una FDP $P(x)$ dada, se puede definir una \textit{función de distribución acumulada} (FDA) $D(x)$ que es útil para muestrear la correspondiente FDP. La FDA $D(x)$ asociada a la FDP $P(x)$ se define como sigue:
	
	\begin{equation*}
	D(x) = \int_{-\infty}^{x}P(t)dt
	\end{equation*}
	
	Para muestrear la FDP $P(x)$ suele utilizarse la inversa de su FDA, $D^{-1}(x)$. Basta generar números reales \textit{pseudo aleatorios} uniformemente distribuidos. Sin embargo, es importante notar que para una FDP arbitraria $P(x)$ no siempre es trivial hallar la inversa de su FDA, $D^{-1}(x)$.
	
	Una de las funciones más populares para utilizar como FDP es la función Gaussiana. Tiene algunas ventajas como por ejemplo que es relativamente fácil de muestrear, sin embargo tiene también desventajas: una funcón Gaussiana sola no sirve para describir una situación donde dos áreas disjuntas del espacio de búsqueda son promisorias, ya que tiene un único máximo. Debido a esto, ACO-$\mathbb{R}$ utiliza una FDP basada en funciones Gaussianas pero ligeramente mejoradas:un \textit{núcleo} Gaussiano.
	
	Un núcleo Gaussiano $G^i(x)$ se define como la suma ponderada de varias funciones Gaussinas unidimensionales $g^i_l(x)$
	
	\begin{equation}
	\label{eq:nucleoGauss}
	G^i(x) = \sum_{l=1}^{k}w_l \cdot g_l^i(x) = \sum_{l=1}^{k}w_l \cdot \frac{1}{\sigma_l^i \cdot \sqrt{2\pi}}\textrm{e}^{-\frac{(x - \mu^i_l)^2}{2{\sigma^i_l}^2}}
	\end{equation}

	ACO-$\mathbb{R}$ utiliza tantos núcleos como variables tiene el CSP, es decir si $n = \vert X \vert$, se usan $n$ núcleos $G^i, i = 1, \dots, n$. Como se observa en la ecuación (\ref{eq:nucleoGauss}), el núcleo $G^i(x)$ está parametrizado con tres vectores de parámetros: $w$ es el vector de pesos asociados con cada función Gaussiana individual, $\mu^i$ es el vector de las medias y $\sigma^i$ el de las desviaciones estándar. La cardinalidad de cada uno de estos vectores es igual al número de funciones Gaussianas que componen el núcleo. Por conveniencia, lo denotaremos con $k$, es decir $\vert w \vert = \vert \mu^i \vert = \vert \sigma^i \vert = k$
	
	Al utilizar un núcleo como FDP seguimos manteniendo la facilidad de muestreo pero obtenemos una flexibilidad en cuanto a la forma que puede tomar si lo comparamos con una función Gaussiana individual. La figura (\ref{fig:gaussianKernel}) muestra un ejemplo de la forma que puede tomar la FDP de un núcleo Gaussiano. 
	
	\begin{figure}
		\includegraphics[width=\linewidth]{gaussianKernel.png}
		\caption{Ejemplo de cinco funciones Gaussianas y su superposición que es el núcleo resultante.}
		\label{fig:gaussianKernel}
	\end{figure}

	
  \begin{thebibliography}{1}
      \bibitem{corr}
      Corral, C.: 
      Designing RC active filters with standard-component values. Electronic Design
      Network Magazine, 141-154 (2000)
      
      \bibitem{lov:rom:per}
      Lovay, M., Romero, E., Peretti, M.:
      Dise\~no de Filtros Activos Robustos usando Algoritmos Genéticos.
      SII 2015 4$^\circ$ Simposio Argentino de Informática Industrial.
      
      \bibitem{dim}
      Dimopoulos, H.: 
      Analog Electronics Filters: Theory, Design and Synthesis. 
      Springer (2012)
      
      \bibitem{rau:swa}
      Raut, R., Swamy, M. N. S.: 
      Modern Analog Filter Analysis and Design: A Practical Approach. 
      Wiley-VCH (2010).
      
      \bibitem{sol}
      Solnon, C.:
      Ant Colony Optimization and Constraint Programming. (Wiley-ISTE 2010) 
      
      \bibitem{soc:dor}
      Socha, K., Dorigo, M.:
      Ant colony optimization for continuous domains.
      European Journal of Operational Research 185 (2008)
      
      \bibitem{dor92}
      Dorigo, M.:
      Optimization, Learning and Natural Algorithms (en italiano). 
      Ph.D. thesis, Dipartimento di Elettronica, Politecnico di Milano, Italy (1992).
      
      \bibitem{dor:man:col}
      Dorigo, M., Maniezzo, V., Colorni, A.: 
      Ant System: Optimization by a colony of cooperating agents. 
      IEEE Transactions on Systems, Man, and Cybernetics – Part B 26 (1), 29–41. (1996)
      
      \bibitem{stu:dor}
      Stützle, T., Dorigo, M.:
      ACO algorithms for the traveling salesman problem. 
      En: Miettinen, K., Mäkelä, M.M., Neittaanmäki, P., Périaux, J. (Eds.), Evolutionary Algorithms in
	  Engineering and Computer Science. John Wiley and Sons,
	  Chichester, UK, pp. 163–183 (1999)

   	\bibitem{dor:schu}
	Dorigo, M., Stützle, T.: 
	Ant Colony Optimization. 
	MIT Press, Cambridge, MA (2004)
      
	  \bibitem{fre:wal}
	  Freuder E., Wallace R.: 
	  Partial constraint satisfaction.
	  Artificial Intelligence, vol. 58, p. 21–70 (1992)
	  
	\bibitem{shi:far:ver}
	Shiex T., Fargier H., Verfaillie G.:
	Valued constraint satisfaction problems: hard and easy problems. 
	International Joint Conference on Artificial Intelligence (IJCAI),MIT Press, Cambridge, Etats-
	Unis, p. 631–637, (1995)
	
	\bibitem{gos:aro:den:pas}
	Goss, S., Aron, S., Deneubourg, J.-L., Pasteels, J.: 
	Self-organized shortcuts in the Argentine ant. 
	Naturwissenschaften 76, 579–581 (1989)
      
      
  \end{thebibliography}

\end{document}
